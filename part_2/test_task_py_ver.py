# -*- coding: utf-8 -*-
"""test_task_py_ver.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N5WYs-rWaOPF1YvA1emcfmZCAXMpbNWh

Эта версия такая же как в тетрадке, только тут закомментены некоторые строки и переписаны принты. Код был сохранен также с колаба и в питоне не запускался, но по идее все должно работать аналогично.

**[Часть 2]** 

    а) лемматизировать текст с помощью mystem или pymorphy2,
    б) найти такие леммы (не исходные словоформы), в которых было бы две (не больше, не меньше) буквы «о», 
    в) программно обратиться к странице (скачать питоном) http://lib.ru/POEZIQ/PESSOA/lirika.txt , 
    г) составить словарь для текста, который там размещен,
    д) записать этот словарь в файл в формате JSON.
"""

#from google.colab import drive
#drive.mount('/content/gdrive') #, force_remount=True

#import os
#os.chdir('gdrive/My Drive/Colab Notebooks')

#pip install pymorphy2

"""# Часть 1"""

#pip install wget

#import wget

#wget.download('https://cloclo15.cloud.mail.ru/weblink/view/V2pn/2yB9UEnz2?etag=1A0C163E53183EAA3076A31BD641FFC40534B7B6', 'cloud_text.txt')

"""Обработка текста и создание словаря"""

f = open('cloud_text.txt', 'r', encoding = 'utf-8-sig')
text = f.read()
f.close()

print(text[:100])

import re

rewords = re.compile('[\\n]', re.DOTALL)
text = re.sub(rewords, ' ', text).lower() #заменяем лишние символы на пробелы, чтобы слова не склеились, и приводим все к нижнему регистру
text = re.sub("[^\s\w]+", " ", text) #убираем всю пунктуцацию

print(text[:100])

import nltk
nltk.download('punkt')

from nltk import word_tokenize

words = word_tokenize(text, language = 'russian')

print(words[:5])

"""или можно выделить слова при помощи регулярных выражений"""

words_2 = [w for w in re.findall("([A-Za-zА-Яа-яЁё]+)", text)]

print(words_2[:5])

"""Составим словарь"""

import collections
c = collections.Counter()
for word in words:
  c[word] += 1

from collections import OrderedDict

f = open('test_text_dict.csv', 'w', encoding='utf-8')
dd = OrderedDict(sorted(dict(c).items(), key=lambda t: t[1], reverse = True))
for i in dd.keys():
  s = i+','+str(dd[i])+'\n'
  f.write(s)
f.close()

"""# Часть 2.1

Лемматизируем слова из получившегося в первой части списка
"""

import pymorphy2
morph = pymorphy2.MorphAnalyzer()

lemmas = [morph.parse(word)[0].normal_form for word in words]

for i in range(10):
  print(words[i], lemmas[i])

f = open("oo_list.txt", 'w', encoding = 'utf-8')
for lemma in lemmas:
  if lemma.count('о') == 2:
    s = lemma + '\n'
    f.write(s)
f.close()

"""# Часть 2.2"""

import requests

r = requests.get('http://lib.ru/POEZIQ/PESSOA/lirika.txt')

print(r.text[:100])

reContents = re.compile('<pre>.*?<pre>', re.DOTALL) #уберем содержание меню в верхнем углу
reTag = re.compile('<.*?>', re.DOTALL) #очистим текст от тегов
text = re.sub(reContents, '', r.text)
text = re.sub(reTag, '', text)

print(text[:100])

nltk.download('stopwords')

from nltk.tokenize import RegexpTokenizer #используем другой токенизатор
from nltk.corpus import stopwords #уберем также стоп-слова

morph = pymorphy2.MorphAnalyzer()
token = RegexpTokenizer('\w+')
stops = set(stopwords.words('russian')) 

def text_to_lemmas(text):
    lemmas = [morph.parse(word)[0].normal_form for word in token.tokenize(text) if morph.parse(word)[0].normal_form not in stops]
    return lemmas

import collections
from collections import OrderedDict

def freq_dict(words):
  c = collections.Counter()
  for word in words:
    c[word] += 1
  
  return OrderedDict(sorted(dict(c).items(), key=lambda t: t[1], reverse = True))

final_dict = freq_dict(text_to_lemmas(text))

import json

with open('test_text_dict_2.json', 'w') as f:
    json.dump(final_dict, f)

